# -*- coding: utf-8 -*-
"""dental .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9wtezdhJwjzmLxFb2NYPaFm2THzsJFR

# Q1

### getting data from url
"""

import pandas as pd

file_id = '1HAAdP4rtI2EkJS_867XHMajXxY6z0sRU'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
df = pd.read_csv(url)

"""Printing some data samples to review its structure and content."""

print(df.T)

"""## part 1

Splitting the data into training and test sets, then creating a decision tree model using the sklearn library. The model is then fitted to the training data, and the precision, accuracy, and recall metrics are printed.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score

X = df.drop('Outcome', axis=1)
y = df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='binary')
recall = recall_score(y_test, y_pred, average='binary')

# Display the metrics
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')

"""## part 2

We should plot the results of tuning various hyperparameters—namely, 'criterion,' 'max_depth,' 'min_samples_split,' and 'min_samples_leaf'—for our decision tree model. To achieve this, we used two methods: (1) grid search, where specific parameters were set manually, and (2) random search, which selects values from defined intervals.

The grid search use the accuracy to search the best parameters. Here we have changed all parameters over our testing.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score

param_grid = {
    'criterion': ['gini','entropy'],
    'max_depth': [5, 30,60],
    'min_samples_split': [10,50,100],
    'min_samples_leaf': [1,10, 20,50]
}

scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, average='binary'),
    'recall': make_scorer(recall_score, average='binary')
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    scoring=scoring,
    refit='accuracy',
    cv=10,
    return_train_score=True
)

grid_search.fit(X_train, y_train)
test_results = []

for params in grid_search.cv_results_['params']:
    model = DecisionTreeClassifier(random_state=42, **params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
    test_recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
    test_results.append({
        'params': params,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision,
        'test_recall': test_recall
    })

test_results_df = pd.DataFrame(test_results)
plt.figure(figsize=(18, 6))

# Plot each metric
plt.plot(test_results_df.index, test_results_df['test_accuracy'], marker='o', label='Test Accuracy', color='blue')
plt.plot(test_results_df.index, test_results_df['test_precision'], marker='o', label='Test Precision', color='orange')
plt.plot(test_results_df.index, test_results_df['test_recall'], marker='o', label='Test Recall', color='green')

# Adding labels and title
plt.xlabel('Model Index')
plt.ylabel('Score')
plt.title('Test Performance of Decision Tree Models with Different Hyperparameters')
plt.xticks(test_results_df.index)
plt.legend()
plt.tight_layout()
plt.show()

for index, row in test_results_df.iterrows():
    print(f"Hyperparameters for model {index}: {row['params']}, Test Accuracy: {row['test_accuracy']:.2f}, "
          f"Test Precision: {row['test_precision']:.2f}, Test Recall: {row['test_recall']:.2f}")

"""printing the parameters for model which has the highest accuracy."""

best_model = test_results_df.loc[test_results_df['test_accuracy'].idxmax()]
print(f"Best Model Hyperparameters: {best_model['params']}")
print(f"Best Model Test Accuracy: {best_model['test_accuracy']:.2f}")
print(f"Best Model Test Precision: {best_model['test_precision']:.2f}")
print(f"Best Model Test Recall: {best_model['test_recall']:.2f}")

"""in this part we just changed two parameters with different values."""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score

param_grid = {
    'criterion': ['entropy'],
    'max_depth': [5,10,20, 30,60,80,100,130,150,200],
    'min_samples_split': [10],
    'min_samples_leaf': [1,5,10, 20,30,50,70,100,120,200]
}

scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, average='binary'),
    'recall': make_scorer(recall_score, average='binary')
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    scoring=scoring,
    refit='accuracy',
    cv=10,
    return_train_score=True
)

grid_search.fit(X_train, y_train)
test_results = []

for params in grid_search.cv_results_['params']:
    model = DecisionTreeClassifier(random_state=42, **params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
    test_recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
    test_results.append({
        'params': params,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision,
        'test_recall': test_recall
    })

test_results_df = pd.DataFrame(test_results)
plt.figure(figsize=(18, 6))

# Plot each metric
plt.plot(test_results_df.index, test_results_df['test_accuracy'], marker='o', label='Test Accuracy', color='blue')
plt.plot(test_results_df.index, test_results_df['test_precision'], marker='o', label='Test Precision', color='orange')
plt.plot(test_results_df.index, test_results_df['test_recall'], marker='o', label='Test Recall', color='green')

# Adding labels and title
plt.xlabel('Model Index')
plt.ylabel('Score')
plt.title('Test Performance of Decision Tree Models with Different Hyperparameters')
plt.xticks(test_results_df.index)
plt.legend()
plt.tight_layout()
plt.show()

for index, row in test_results_df.iterrows():
    print(f"Hyperparameters for model {index}: {row['params']}, Test Accuracy: {row['test_accuracy']:.2f}, "
          f"Test Precision: {row['test_precision']:.2f}, Test Recall: {row['test_recall']:.2f}")


import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from skopt import BayesSearchCV

# Define the parameter space for Bayesian optimization
param_space = {
    'criterion': ['gini','entropy'],
    'max_depth': (5, 100),  # Range for max_depth
    'min_samples_split': (2, 100),  # Range for min_samples_split
    'min_samples_leaf': (1, 60)  # Range for min_samples_leaf
}

# Initialize BayesSearchCV
bayes_search = BayesSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    search_spaces=param_space,
    n_iter=60,  # Number of parameter settings sampled
    scoring='accuracy',  # Only refitting based on accuracy
    cv=15,
    random_state=42
)

# Fit BayesSearchCV to find best parameters
bayes_search.fit(X_train, y_train)

# Prepare to collect test metrics for each model
test_metrics = []

# Evaluate each model from the search on test data
for i, params in enumerate(bayes_search.cv_results_['params']):
    # Train model on train data with the given parameters
    model = DecisionTreeClassifier(random_state=42, **params)
    model.fit(X_train, y_train)

    # Predict on test data and calculate metrics
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')

    # Store metrics along with model's parameters
    test_metrics.append({
        'model_num': i + 1,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'params': params
    })

# Convert test results to DataFrame
test_metrics_df = pd.DataFrame(test_metrics)

# Plot test metrics for each model
plt.figure(figsize=(18, 6))
plt.plot(test_metrics_df['model_num'], test_metrics_df['accuracy'], marker='o', label='Accuracy')
plt.plot(test_metrics_df['model_num'], test_metrics_df['precision'], marker='o', label='Precision')
plt.plot(test_metrics_df['model_num'], test_metrics_df['recall'], marker='o', label='Recall')
plt.title('Test Metrics for Each Model')
plt.xlabel('Model Number')
plt.ylabel('Score')
plt.legend()
plt.grid()
plt.show()

# Display test results with parameters for reference
print(test_metrics_df[['model_num', 'accuracy', 'precision', 'recall', 'params']])